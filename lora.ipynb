{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PertModel' from 'peft' (/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/peft/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, TaskType, PertModel\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Check if GPU is available\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PertModel' from 'peft' (/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/peft/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PertModel\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "base_model=\"distilgpt2\"\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "tokenised_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenised_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 147,456 || all params: 82,060,032 || trainable%: 0.1797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-5): 6 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Wrap model with LoRA adapter\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/transformers/training_args.py:2262: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2295' max='2295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2295/2295 15:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.431100</td>\n",
       "      <td>1.552422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2295, training_loss=2.098729048003818, metrics={'train_runtime': 960.0946, 'train_samples_per_second': 38.244, 'train_steps_per_second': 2.39, 'total_flos': 1203444935294976.0, 'train_loss': 2.098729048003818, 'epoch': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model/',\n",
    "    eval_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='model/logs',\n",
    "    logging_steps=10,\n",
    "    # no_cuda=True,\n",
    "    use_mps_device=True,  #VERY IMPORTANT PARAM\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_datasets['train'],\n",
    "    eval_dataset=tokenised_datasets['validation']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# trainer.train(resume_from_checkpoint='model/checkpoint-8500')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/trained_model/tokenizer_config.json',\n",
       " 'model/trained_model/special_tokens_map.json',\n",
       " 'model/trained_model/vocab.json',\n",
       " 'model/trained_model/merges.txt',\n",
       " 'model/trained_model/added_tokens.json',\n",
       " 'model/trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# save the model and tokenizer explicitly\n",
    "model_output_dir = 'model/trained_model'\n",
    "\n",
    "model.save_pretrained(model_output_dir)\n",
    "tokenizer.save_pretrained(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/nirmal/Desktop/llm fine tuning/env_llm/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 82060032\n",
      "\n",
      " Generated:\n",
      "Once upon a time, he began to think about how people can learn and learn from their own mistakes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "base_model=\"distilgpt2\"\n",
    "\n",
    "def get_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def generate_text(input_text):\n",
    "    model_path = \"model/trained_model\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    base = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "    model = PeftModel.from_pretrained(base, model_path)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Total parameters: {get_model_parameters(model)}\")\n",
    "\n",
    "    inputs = tokenizer(input_text, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=125,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n Generated:\")\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "generate_text(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Base model output:\n",
      "Once upon a time of such turmoil and the destruction of her homeland, her father, the first of her sons, is the only son to be killed in her absence. The young princess is named after the Great King.\n",
      "\n",
      "\n",
      "ðŸ”§ LoRA fine-tuned model output:\n",
      "Once upon a time of peace, a great deal of hatred has been formed in the Jewish community, which is more concerned with survival than the Jews themselves. In the 1920s, there was widespread hatred of Jews and Jews, and the hatred that surrounded them grew. In 1922, in response to the publication of the book \"The Jews in the Jews: The Jewish World,\" Jewish newspapers published a series of articles criticizing Jewish prejudice. In 1925, the editor-in-chief of The Jewish World published a cover story on Zionism: a series of articles accusing Jews of being \"Jewish\" and that Jews were not \"Jews,\"\n",
      "/n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "base_model=\"distilgpt2\"\n",
    "\n",
    "def compare_models(prompt: str, max_length=125):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # Load base model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    base_model_instance = AutoModelForCausalLM.from_pretrained(base_model).to(device)\n",
    "\n",
    "    # Load fine-tuned LoRA model\n",
    "    from peft import PeftModel\n",
    "    lora_model = PeftModel.from_pretrained(base_model_instance, \"model/trained_model\").to(device)\n",
    "\n",
    "    # Tokenise input once\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate from base model\n",
    "    base_output = base_model_instance.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    # Generate from LoRA model\n",
    "    lora_output = lora_model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    # Decode outputs\n",
    "    base_text = tokenizer.decode(base_output[0], skip_special_tokens=True)\n",
    "    lora_text = tokenizer.decode(lora_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print both\n",
    "    print(\"ðŸ“¦ Base model output:\")\n",
    "    print(base_text)\n",
    "    print(\"ðŸ”§ LoRA fine-tuned model output:\")\n",
    "    print(lora_text)\n",
    "\n",
    "compare_models(\"Once upon a time\")\n",
    "compare_models(\"== Early Life ==\\nJohn Keats was born in\")\n",
    "compare_models(\"== Background ==\\nThe Battle of Hastings was\")\n",
    "compare_models(\"== Legacy ==\\nEinstein's work influenced\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model perplexity: 53.63\n",
      "LoRA fine-tuned model perplexity: 53.63\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "base_model=\"distilgpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def compute_perplexity(model, tokenizer, text, device):\n",
    "    encodings = tokenizer(text, return_tensors='pt')\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        perplexity = math.exp(loss.item())\n",
    "        return perplexity\n",
    "    \n",
    "# Load base model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "base_model_instance = AutoModelForCausalLM.from_pretrained(base_model).to(device)\n",
    "\n",
    "# Load fine-tuned LoRA model\n",
    "from peft import PeftModel\n",
    "lora_model = PeftModel.from_pretrained(base_model_instance, \"model/trained_model\").to(device)\n",
    "text = dataset[\"validation\"][10][\"text\"]\n",
    "\n",
    "base_ppl = compute_perplexity(base_model_instance, tokenizer, text, device)\n",
    "lora_ppl = compute_perplexity(lora_model, tokenizer, text, device)\n",
    "\n",
    "print(f\"Base model perplexity: {base_ppl:.2f}\")\n",
    "print(f\"LoRA fine-tuned model perplexity: {lora_ppl:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
